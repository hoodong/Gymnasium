
5.성능 향상 기법
몬테카를로 방법에서 속도 및 수렴을 향상 방법

5-1.점증 계산법
- 앞에서 다룬 몬테카를로 알고리즘에서는 데이터를 추가할 때마다 평균을 다시 계산했음  
  $A_t = \frac{R_1+\cdots+R_t}{t}$  
  $A_{t+1} = \frac{R_1+\cdots+R_t+R_{t+1}}{t+1}$    
- 이 방식은 중복 계산이 많아 비효율적이며 모든 괘적을 저장해야 하므로 메모리 부담이 큼
- 점증 계산법(incremental computation)은 이전 평균값과 추가된 데이터만 사용해 새로운 평균값을 계산함  
  $A_{t+1} = A_t + \frac{R_{t+1}-A_t}{t+1}$

5-2.오프 정책
- 온 정책(on-policy)은 현재 정책에서 행동 가치를 추정하고 이를 사용해 정책을 개선
- 즉 가치 함수를 계산할 때 최신 정책이 생성한 에피소드를 사용
- 온 정책은 현재 정책에 의존하므로 탐사에 치우칠 우려가 있고, 이를 피하기 위해 $\epsilon$-소프트 기법 등을 사용
- 오프 정책(off-policy)는 탐험과 탐사의 균형을 이루는 또다른 방법으로서, 두 개의 정책을 사용
- 목표 정책(target policy)은 찾고자 하는 정책이고, 행동 정책(behavior policy)은 에피소드를 생성할 때 사용
- 중요도 샘플링(importance sampling) 기법을 활용하면, 행동정책에서 얻은 에피소드로부터 목표정책의 가치함수를 갱신할 수 있음

